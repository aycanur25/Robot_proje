# Reinforcement Learning with Mujoco-Pytorch

Bu proje, **Mujoco** ortamlarÄ±nda **PekiÅŸtirmeli Ã–ÄŸrenme (Reinforcement Learning)** algoritmalarÄ±nÄ± kullanarak yapÄ±lan deneysel Ã§alÄ±ÅŸmalarÄ± iÃ§ermektedir. KullanÄ±lan algoritmalar ve test edilen ortamlar ÅŸunlardÄ±r:

- **Proximal Policy Optimization (PPO)**
- **Q-Learning**
- **Soft Actor-Critic (SAC)**
- **Deep Deterministic Policy Gradient (DDPG)**

## ğŸ§‘â€ğŸ’» KullanÄ±lan Algoritmalar

- **PPO (Proximal Policy Optimization)**: Proximal Policy Optimization, gÃ¼venli politika gÃ¼ncellemeleri saÄŸlayarak daha istikrarlÄ± Ã¶ÄŸrenme sÃ¼reci sunar.
- **Q-Learning**: Q-Learning, deÄŸer tabanlÄ± bir pekiÅŸtirmeli Ã¶ÄŸrenme algoritmasÄ±dÄ±r. Bu projede **Mountain Car** ortamÄ±nda uygulandÄ±.
- **SAC (Soft Actor-Critic)**: SAC, entropi temelli bir algoritmadÄ±r ve keÅŸif ile istikrarÄ± dengesiz bir ÅŸekilde saÄŸlar. **Mountain Car** ortamÄ±nda kullanÄ±ldÄ±.
- **DDPG (Deep Deterministic Policy Gradient)**: SÃ¼rekli aksiyon alanlarÄ±yla Ã§alÄ±ÅŸmak iÃ§in kullanÄ±lan bir algoritmadÄ±r. **Mountain Car** ortamÄ±nda uygulandÄ±.

## ğŸï¸ Ortamlar

Projede test edilen ortamlar aÅŸaÄŸÄ±daki gibidir:

- **Half Cheetah** (Mujoco)
- **Mountain Car** (OpenAI Gym)

## ğŸ“ˆ Performans

AlgoritmalarÄ±n her biri, ilgili ortamlar Ã¼zerinde eÄŸitim ve test sÃ¼reÃ§lerinden geÃ§irilmiÅŸtir. EÄŸitim sÃ¼recinin gÃ¶rselleÅŸtirilmesi saÄŸlanmÄ±ÅŸtÄ±r ve performans sonuÃ§larÄ± izlenebilir.

## ğŸš€ BaÅŸlangÄ±Ã§

### Gereksinimler

Bu projeyi Ã§alÄ±ÅŸtÄ±rmak iÃ§in aÅŸaÄŸÄ±daki gereksinimlere ihtiyacÄ±nÄ±z olacak:

- Python 3.x
- gym 0.17.x+
- mujoco_py 2.0.x+
- pytorch 1.6.x+

### Kurulum

Projeyi Ã§alÄ±ÅŸtÄ±rmaya baÅŸlamak iÃ§in aÅŸaÄŸÄ±daki adÄ±mlarÄ± izleyin:

1. Repo'yu klonlayÄ±n:

    ```bash
    git clone https://github.com/<repo_adÄ±>.git
    cd <repo_adÄ±>
    ```

2. Gerekli kÃ¼tÃ¼phaneleri yÃ¼kleyin:

    ```bash
    pip install -r requirements.txt
    ```

### Mujoco Kurulumu

**Mujoco** ortamlarÄ± iÃ§in `mujoco_py` kÃ¼tÃ¼phanesinin kurulmasÄ± gerekmektedir. Bu, simÃ¼lasyonlarÄ± Ã§alÄ±ÅŸtÄ±rmak iÃ§in gereklidir. Kurulum talimatlarÄ±nÄ± [Mujoco](https://github.com/openai/mujoco) ve [mujoco_py](https://github.com/openai/mujoco-py) reposunda bulabilirsiniz.

Mujoco'yu kurduktan sonra aÅŸaÄŸÄ±daki adÄ±mlarÄ± izleyerek ortamlarÄ± Ã§alÄ±ÅŸtÄ±rabilirsiniz.

1. **Mujoco**'yu indirip kurun ve Ã§evresel deÄŸiÅŸkenlerinizi ayarlayÄ±n:
   
   ```bash
   export MUJOCO_PY_MUJOCO_PATH=/path/to/mujoco
   export MUJOCO_PY_MJKEY_PATH=/path/to/mjkey.txt

# Parametreler:
--env_name: KullanÄ±lacak ortam adÄ±. (default: 'MountainCar-v0')

'Ant-v2', 'HalfCheetah-v2', 'Hopper-v2', 'Humanoid-v2', 'HumanoidStandup-v2', 'InvertedDoublePendulum-v2', 'InvertedPendulum-v2', 'Walker2d-v2', 'Swimmer-v2', 'Reacher-v2'

--algorithm: SeÃ§ilen algoritma. ('PPO', 'Q-Learning', 'SAC', 'DDPG')

--train: EÄŸitim aÅŸamasÄ±nÄ± baÅŸlatÄ±r. (default: True)

--render: OrtamÄ± gÃ¶rselleÅŸtirir. (default: False)

--epochs: EÄŸitim sÃ¼resi, toplam epoch sayÄ±sÄ±. (default: 1000)

--entropy_coef: Entropi katsayÄ±sÄ±. (default: 0.01)

--critic_coef: Critic katsayÄ±sÄ±. (default: 0.5)

--learning_rate: Ã–ÄŸrenme oranÄ±. (default: 0.0003)

--gamma: Ä°ndirim faktÃ¶rÃ¼. (default: 0.99)

--lmbda: GAE'de kullanÄ±lan lambda katsayÄ±sÄ±. (default: 0.95)

--eps_clip: AktÃ¶r ve kritik aÄŸlarÄ±nÄ±n clip aralÄ±ÄŸÄ±. (default: 0.2)

--K_epoch: EÄŸitimde kullanÄ±lan epoch sayÄ±sÄ±. (default: 64)

--T_horizon: Bir jenerasyonun eÄŸitime baÅŸlamadan Ã¶nceki zaman adÄ±mlarÄ±. (default: 2048)

--hidden_dim: AktÃ¶r ve kritik aÄŸlarÄ±nÄ±n gizli katman boyutu. (default: 64)

--minibatch_size: Mini-batch boyutu. (default: 64)

--tensorboard: TensorBoard desteÄŸi. (default: False)

--load: YÃ¼klenmesi gereken model ismi. (default: 'no')

--save_interval: Modelin kaydedilme sÄ±klÄ±ÄŸÄ±. (default: 100)

--print_interval: EÄŸitim sÄ±rasÄ±nda yazdÄ±rma sÄ±klÄ±ÄŸÄ±. (default: 20)

--use_cuda: CUDA kullanÄ±m durumu. (default: True)
